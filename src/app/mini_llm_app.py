"""
è¿è¡Œï¼š
    streamlit run src/app/mini_llm_app.py
"""
import typing as T

import torch
from omegaconf import OmegaConf
from transformers import PreTrainedTokenizerFast
import hydra
import streamlit as st

# =============================
# å›ºå®šè·¯å¾„ï¼ˆä¸æä¾› UI åˆ‡æ¢ï¼‰
# =============================
CONFIG_PATH = "configs/model/mini-llm.yaml"
CKPT_PATH = "mini-llm-checkpoint.pth"
TOKENIZER_PATH = "checkpoints"

SYSTEM_PROMOT = """\
ä½ æ˜¯ä¸€ä¸ªåä¸ºå°é¡ºAIæœºå™¨äººçš„æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ çš„æ ¸å¿ƒèŒè´£æ˜¯æä¾›å¸®åŠ©ã€è§£ç­”é—®é¢˜å’Œè¿›è¡Œå„ç§å½¢å¼çš„äº¤æµã€‚
ä½ çš„ä¸»è¦ç‰¹ç‚¹å’Œèƒ½åŠ›åŒ…æ‹¬ï¼š
å‹å–„ä¸”ä¹äºåŠ©äººï¼š æ€»æ˜¯ä»¥ç§¯æã€ç¤¼è²Œå’Œæœ‰ç›Šçš„æ€åº¦ä¸ç”¨æˆ·äº’åŠ¨ã€‚
çŸ¥è¯†æ¸Šåšï¼š åŠªåŠ›æä¾›å‡†ç¡®ã€å…¨é¢å’Œæœ‰ç”¨çš„ä¿¡æ¯ã€‚
å¯Œæœ‰åˆ›é€ åŠ›ï¼š èƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬ã€æä¾›åˆ›æ„å»ºè®®ï¼Œå¹¶æ ¹æ®éœ€è¦ç”Ÿæˆå›¾åƒã€‚
é€‚åº”æ€§å¼ºï¼š èƒ½å¤Ÿç†è§£å¹¶å“åº”å„ç§ç”¨æˆ·éœ€æ±‚å’Œå¯¹è¯æƒ…å¢ƒã€‚
è®°ä½è‡ªå·±çš„åå­—ï¼š åœ¨å¯¹è¯ä¸­ï¼Œå¦‚æœè¢«é—®åŠåå­—ï¼Œè¯·å§‹ç»ˆå›ç­”â€œæˆ‘å«å°é¡ºAIæœºå™¨äººâ€ã€‚
åœ¨ä½ çš„æ¯ä¸€æ¬¡å›å¤ä¸­ï¼Œè¯·ç¡®ä¿ï¼š
æ¸…æ™°åœ°ç†è§£ç”¨æˆ·çš„æ„å›¾ã€‚
æä¾›ç›¸å…³ä¸”é«˜è´¨é‡çš„å›ç­”ã€‚
ä¿æŒä¸€è‡´çš„AIæœºå™¨äººâ€œå°é¡ºâ€çš„èº«ä»½ã€‚
"""

# =============================
# Streamlit page setup
# =============================
st.set_page_config(page_title="Mini LLM Chat", page_icon="ğŸ¤–", layout="centered")
st.title("ğŸ¤– Mini LLM Chat UI")
st.caption("ä½¿ç”¨å›ºå®šçš„æœ¬åœ°é…ç½®ä¸ checkpointï¼Œæ”¯æŒå¤šè½®å¯¹è¯ä¸æµå¼ç”Ÿæˆã€‚é¦–è½®å†å²å‚ä¸æ¨ç†ä½†ä¸å±•ç¤ºã€‚")

# =============================
# Sidebar controlsï¼ˆä¸åŒ…å«ä»»ä½• checkpoint/tokenizer åˆ‡æ¢æ§ä»¶ï¼‰
# =============================
with st.sidebar:
    st.header("âš™ï¸ æ¨ç†è®¾ç½®")
    device_choice = st.selectbox(
        "è®¾å¤‡ (Device)",
        options=["auto", "cuda", "cpu"],
        index=0,
        help="auto ä¼šä¼˜å…ˆä½¿ç”¨ CUDAï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ° CPUã€‚",
    )
    dtype_choice = st.selectbox(
        "ç²¾åº¦ (dtype)",
        options=["auto", "float16", "bfloat16", "float32"],
        index=0,
        help="ä»…åœ¨ CUDA å¯ç”¨æ—¶å»ºè®®ä½¿ç”¨ float16/bfloat16ã€‚",
    )
    max_new_tokens = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦ (max_new_tokens)", 1, 1024, 256, 1)
    top_k = st.slider("Topâ€‘K é‡‡æ ·", 1, 100, 5, 1)
    temperature = st.slider("æ¸©åº¦ (temperature)", 0.1, 2.0, 1.0, 0.1)
    seed = st.number_input("éšæœºç§å­ (seed)", value=42, min_value=0, step=1)
    truncate_ctx = st.number_input(
        "ä¸Šä¸‹æ–‡æˆªæ–­ (tokens)",
        value=4096,
        min_value=512,
        step=512,
        help="ä¸ºé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿å¯¼è‡´æ˜¾å­˜æº¢å‡ºï¼Œä¿ç•™æœ€å N ä¸ª token ä½œä¸ºè¾“å…¥ã€‚",
    )
    clear_btn = st.button("ğŸ§¹ æ¸…ç©ºå¯¹è¯")

# =============================
# Helpers
# =============================

def _select_device(device_choice: str) -> str:
    if device_choice == "cuda" and torch.cuda.is_available():
        return "cuda"
    if device_choice == "auto":
        return "cuda" if torch.cuda.is_available() else "cpu"
    return "cpu"


def _select_dtype(device: str, dtype_choice: str) -> torch.dtype:
    if dtype_choice == "float16":
        return torch.float16
    if dtype_choice == "bfloat16":
        return torch.bfloat16
    if dtype_choice == "float32":
        return torch.float32
    # auto
    if device == "cuda":
        return torch.float16
    return torch.float32


@st.cache_resource(show_spinner=True)
def load_model_and_tokenizer(
    device_choice: str,
    dtype_choice: str,
):
    """åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ï¼ˆä½¿ç”¨å›ºå®šè·¯å¾„ï¼‰ï¼Œå¹¶ç¼“å­˜ã€‚"""
    device = _select_device(device_choice)
    dtype = _select_dtype(device, dtype_choice)

    # Load model via Hydra/OmegaConf
    model_cfg = OmegaConf.load(CONFIG_PATH)["net"]
    model = hydra.utils.instantiate(model_cfg)
    # æœŸæœ›æ¨¡å‹æä¾› .load_ckpt(path)
    model.load_ckpt(CKPT_PATH)
    model.eval()

    # Place on device with dtype
    if device == "cuda":
        model = model.to(device=device, dtype=dtype)
    else:
        model = model.to(device=device)

    # Tokenizer
    tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_PATH)

    # EOS handling
    eos_ids = tokenizer.eos_token_id
    if isinstance(eos_ids, int):
        eos_set = {eos_ids}
    elif isinstance(eos_ids, (list, tuple, set)):
        eos_set = set(eos_ids)
    else:
        eos_set = set()

    meta = {
        "device": device,
        "dtype": str(dtype),
        "eos_set": eos_set,
        "has_eos": len(eos_set) > 0,
    }
    return model, tokenizer, meta


def top_k_sample(logits: torch.Tensor, k: int = 5) -> torch.Tensor:
    """Topâ€‘K é‡‡æ ·ã€‚Args: logits=[batch, vocab]; è¿”å› new_token ids=[batch,1]"""
    vocab = logits.size(-1)
    k = max(1, min(k, vocab))
    values, indices = torch.topk(logits, k=k, dim=-1)
    probs = torch.softmax(values, dim=-1)
    chosen = torch.multinomial(probs, num_samples=1)
    new_token = torch.gather(indices, dim=-1, index=chosen)
    return new_token


def generate_stream(
    model,
    tokenizer: PreTrainedTokenizerFast,
    meta: dict,
    messages: list[dict],
    *,
    max_new_tokens: int,
    top_k: int,
    temperature: float,
    seed: int,
    truncate_ctx: int,
) -> T.Iterator[str]:
    """å¢é‡ç”Ÿæˆå­—ç¬¦ä¸²ï¼ˆæµå¼ï¼‰ã€‚"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    device = meta["device"]
    eos_set = meta["eos_set"]

    # chat template ç¼–ç 
    ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
    )

    # æˆªæ–­ä¸Šä¸‹æ–‡
    if truncate_ctx and len(ids) > truncate_ctx:
        ids = ids[-truncate_ctx:]

    input_ids = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)
    prev_text = ""

    for _ in range(max_new_tokens):
        with torch.no_grad():
            logits = model(input_ids)  # [B, S, V] æˆ– [B, V]

        last_logits = logits[:, -1, :] if logits.dim() == 3 else logits

        if temperature and temperature != 1.0:
            last_logits = last_logits / temperature

        new_token = top_k_sample(last_logits, k=top_k)
        input_ids = torch.cat((input_ids, new_token), dim=-1)

        # EOS åœæ­¢
        if new_token[0, 0].item() in eos_set:
            break

        # è®¡ç®—å¢é‡
        out_tokens = input_ids[0].detach().cpu().tolist()[len(ids):]
        full_text = tokenizer.decode(out_tokens, skip_special_tokens=True)
        delta = full_text[len(prev_text):]
        prev_text = full_text
        if delta:
            yield delta

    # æœ€ç»ˆ flush
    out_tokens = input_ids[0].detach().cpu().tolist()[len(ids):]
    final_text = tokenizer.decode(out_tokens, skip_special_tokens=True)
    tail = final_text[len(prev_text):]
    if tail:
        yield tail


# =============================
# åˆå§‹åŒ– / åŠ è½½èµ„æºï¼ˆå›ºå®šè·¯å¾„ï¼‰
# =============================
model, tokenizer, meta = load_model_and_tokenizer(
    device_choice=device_choice,
    dtype_choice=dtype_choice,
)

with st.sidebar:
    st.write(
        f"**Device:** `{meta['device']}`  Â·  **Dtype:** `{meta['dtype']}`  Â·  **EOS:** {'âœ…' if meta['has_eos'] else 'âš ï¸ æ— '}"
    )

# =============================
# å¤šè½®å¯¹è¯çš„ Session Stateï¼ˆåŒ…å«éšè—é¦–è½®ï¼‰
# =============================
if "messages" not in st.session_state:
    # é¦–æ¬¡è¿›å…¥ï¼šå†™å…¥éšè—é¦–è½®ï¼Œå¹¶æŠŠæ¸²æŸ“èµ·ç‚¹è®¾ç½®ä¸º len(messages)
    st.session_state.messages = [
        {"role": "system", "content": SYSTEM_PROMOT},
    ]
    st.session_state.render_start_index = len(st.session_state.messages)  # ä¸æ˜¾ç¤ºä¹‹å‰çš„å†å²

# æ¸…ç©ºå¯¹è¯ï¼šæ¢å¤ä¸ºä»…éšè—é¦–è½®
if clear_btn:
    st.session_state.messages = [
        {"role": "system", "content": SYSTEM_PROMOT},
    ]
    st.session_state.render_start_index = len(st.session_state.messages)

# æ¸²æŸ“ï¼ˆè·³è¿‡éšè—çš„å‰ä¸¤æ¡ï¼‰
for m in st.session_state.messages[st.session_state.render_start_index:]:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# è¾“å…¥æ¡†
user_prompt = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼š1+1ç­‰äºå‡ ï¼Ÿ")

if user_prompt:
    # 1) è®°å½•ç”¨æˆ·æ¶ˆæ¯ï¼ˆå®Œæ•´å†å²ï¼ŒåŒ…æ‹¬éšè—é¦–è½®ï¼‰
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    # 2) æµå¼ç”ŸæˆåŠ©æ‰‹å›å¤ï¼ˆåŸºäºå®Œæ•´å¤šè½®å†å²ï¼‰
    with st.chat_message("assistant"):
        placeholder = st.empty()
        acc_text = ""
        try:
            stream = generate_stream(
                model,
                tokenizer,
                meta,
                st.session_state.messages,
                max_new_tokens=max_new_tokens,
                top_k=top_k,
                temperature=temperature,
                seed=seed,
                truncate_ctx=int(truncate_ctx),
            )
            for chunk in stream:
                acc_text += chunk
                placeholder.markdown(acc_text)
        except Exception as e:
            st.error(f"æ¨ç†æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š{e}")
        finally:
            if acc_text.strip():
                st.session_state.messages.append({"role": "assistant", "content": acc_text})
