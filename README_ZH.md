<div align="center">
<h1>ğŸ§ ğŸ”¬ mini-llm â€” ä»é›¶æ„å»ºå¤§è¯­è¨€æ¨¡å‹</h1>
</div>

<p align="center">
	<a href="README.md"><img alt="English" src="https://img.shields.io/badge/English-d9d9d9"></a>
	<a href="README_ZH.md"><img alt="ä¸­æ–‡" src="https://img.shields.io/badge/%E4%B8%AD%E6%96%87-d9d9d9"></a>
	<br/>
</p>

<p>
	<a href="https://www.python.org/"><img alt="Python 3.11" src="https://img.shields.io/badge/Python-3.11-blue?logo=python"></a>
	<a href="https://pytorch.org/"><img alt="PyTorch 2.8" src="https://img.shields.io/badge/PyTorch-2.8-%23EE4C2C?logo=pytorch"></a>
	<a href="https://lightning.ai/"><img alt="Lightning 2.5.5" src="https://img.shields.io/badge/Lightning-2.5.5-%23792EE5?logo=lightning"></a>
	<a href="https://github.com/huggingface/transformers"><img alt="Transformers 4.56" src="https://img.shields.io/badge/Transformers-4.56-%23FF6F00?logo=huggingface"></a>
	<a href="https://hydra.cc/"><img alt="Hydra 1.3" src="https://img.shields.io/badge/Hydra-1.3-%23000000?logo=dropbox-paper"></a>
	<a href="LICENSE"><img alt="License: Apache-2.0" src="https://img.shields.io/badge/License-Apache_2.0-green.svg"></a>
	<a href="https://github.com/caixiaoshun/mini-llm"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/caixiaoshun/mini-llm"></a>
	<a href="https://github.com/caixiaoshun/mini-llm"><img alt="GitHub Forks" src="https://img.shields.io/github/forks/caixiaoshun/mini-llm"></a>
</p>

<p>
	<a href="https://colab.research.google.com/drive/1EwVictfkx6OUlu3yDuKfebvxeVUNEdbg?usp=sharing"><img alt="åœ¨ Colab æ‰“å¼€ mini-llm" src="https://colab.research.google.com/assets/colab-badge.svg"></a>
	<a href="https://colab.research.google.com/drive/1hoLR-S-YeCS01CVh4x8qg25cpSmGVcxu?usp=sharing"><img alt="åœ¨ Colab æ‰“å¼€ mini-moe" src="https://colab.research.google.com/assets/colab-badge.svg"></a>
	<a href="https://huggingface.co/spaces/caixiaoshun/mini-llm"><img alt="åœ¨ Hugging Face Spaces æ‰“å¼€" src="https://img.shields.io/badge/HuggingFace-Spaces-ffcc4d?logo=huggingface"></a>
	<a href="https://huggingface.co/caixiaoshun/mini-llm"><img alt="Hugging Face Model" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue"></a>
</p>

<a href="https://huggingface.co/spaces/caixiaoshun/mini-llm">
	<img src="assets/mini-llm-cover.png" alt="mini-llm å°é¢å›¾" />
</a>
<br/>

## ğŸ†• æœ‰ä»€ä¹ˆæ–°ç‰¹æ€§

- ğŸ§© ç¨€ç–ä¸“å®¶ MoEï¼šç¨€ç– MoE å‰é¦ˆç½‘ç»œã€å¸¦å™ª Topâ€‘K è·¯ç”±ã€å‡è¡¡è¾…åŠ©æŸå¤±ï¼ˆç›¸å¯¹å‡åŒ€çš„ KLï¼‰ã€‚
- ğŸ“š æ•°æ®æµæ°´çº¿ï¼šæ— ç›‘ç£é¢„è®­ç»ƒï¼ˆä¸‹ä¸€ä¸ª token é¢„æµ‹ï¼‰ä¸æœ‰ç›‘ç£æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼ŒåŸºäº chat-templateï¼‰ã€‚
- âš¡ è®­ç»ƒæŠ€æœ¯æ ˆï¼šPyTorch + Lightning + Hydra é…ç½®ç®¡ç†ï¼Œæ”¯æŒ TensorBoard å¯è§†åŒ–ã€‚
- ğŸš€ æ¨ç†ä¸æ¼”ç¤ºï¼šæœ€å°å¯ç”¨çš„ Python æ¨ç†ç¤ºä¾‹ + Streamlit èŠå¤©åº”ç”¨ã€‚
- ğŸ“¦ æ¨¡å‹å¤§å°ï¼šmini-llm â‰ˆ 171â€¯MBï¼Œmini-moe â‰ˆ 876â€¯MBã€‚

![mini-llm 171MB](https://img.shields.io/badge/mini--llm-171MB-2ea44f)
![mini-moe 876MB](https://img.shields.io/badge/mini--moe-876MB-2ea44f)

éå¸¸é€‚åˆå­¦ä¹ ç«¯åˆ°ç«¯çš„ LLM è®­ç»ƒé—­ç¯ï¼šåˆ†è¯/æ•°æ® â†’ æ¨¡å‹ â†’ è®­ç»ƒ â†’ è¯„ä¼° â†’ æ¨ç†/éƒ¨ç½²ã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„é€Ÿè§ˆ

```
configs/                 # Hydra é…ç½®ï¼ˆæ•°æ®ã€æ¨¡å‹ã€è®­ç»ƒå™¨ã€å®éªŒç»„åˆç­‰ï¼‰
scripts/                 # æ•°æ®ä¸‹è½½ã€åˆ†è¯å™¨è®­ç»ƒç­‰è„šæœ¬
src/
  app/                   # Streamlit èŠå¤©æ¼”ç¤º
  train.py               # è®­ç»ƒå…¥å£ï¼ˆHydraï¼‰
  eval.py                # è¯„ä¼°å…¥å£ï¼ˆHydraï¼‰
tests/                   # åŸºç¡€æµ‹è¯•
requirements.txt         # ä¾èµ–
```

## ğŸ› ï¸ ç¯å¢ƒä¸å®‰è£…

æ¨èï¼šPython 3.11ï¼›è‹¥ç”¨ GPU è®­ç»ƒï¼Œå®‰è£…åŒ¹é…çš„ CUDA ç‰ˆæœ¬ã€‚

1) åˆ›å»ºå¹¶æ¿€æ´» conda ç¯å¢ƒï¼ˆPython 3.11ï¼Œç¯å¢ƒå mini-llmï¼‰ï¼š

```bash
conda create -n mini-llm python=3.11 -y
conda activate mini-llm
```

2) å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

3) éªŒè¯ PyTorch æ˜¯å¦èƒ½æ£€æµ‹åˆ° GPUï¼ˆå¯é€‰ï¼‰ï¼š

```bash
python -c "import torch; print('cuda?', torch.cuda.is_available(), 'num', torch.cuda.device_count())"
```

## ğŸ“šğŸ”¤ æ•°æ®ä¸åˆ†è¯å™¨

æœ¬é¡¹ç›®åŒ…å«ä¸¤ç±»æ•°æ®é›†ï¼š

æ•°æ®é›†è¯´æ˜ï¼š

- ddzhu123/seq-monkeyï¼ˆä¸­æ–‡é€šç”¨å¼€æ”¾è¯­æ–™ï¼‰ï¼šå¤šé¢†åŸŸæ–‡æœ¬ï¼ˆç½‘é¡µã€è®ºå›ã€ç™¾ç§‘ç­‰ï¼‰ï¼Œå·²æ¸…æ´—å»é‡ï¼Œé€‚åˆè‡ªå›å½’é¢„è®­ç»ƒï¼›è„šæœ¬å¯¼å‡ºä¸º `data/mobvoi_seq_monkey_general_open_corpus.jsonl`ã€‚
- BelleGroup/train_3.5M_CNï¼ˆä¸­æ–‡æŒ‡ä»¤å¾®è°ƒè¯­æ–™ï¼‰ï¼šçº¦ 350 ä¸‡æ¡å¤šè½®å¯¹è¯ï¼Œè¦†ç›– QAã€å†™ä½œã€æ¨ç†ã€ç¼–ç¨‹ç­‰ï¼›é€‚åˆ SFTï¼›æœ¬ä»“ä»…å¯¹ assistant ç‰‡æ®µè®¡ç®—æŸå¤±ã€‚è¯·éµå¾ªæ•°æ®é›†è®¸å¯å¹¶æŒ‰éœ€è¿‡æ»¤æ¸…æ´—ã€‚

### âš™ï¸ ä¸€é”®è„šæœ¬ï¼ˆæ¨èåœ¨ WSL/Git Bashï¼‰

å‚è§ `scripts/download-data.sh` è·å–è‡ªåŠ¨åŒ–ä¸‹è½½æ­¥éª¤ã€‚

### ğŸ”¤ è®­ç»ƒåˆ†è¯å™¨ï¼ˆBPE + ByteLevelï¼‰

```bash
python scripts/train_tokenizer.py
```

è¾“å‡ºä¿å­˜åœ¨ `checkpoints/` ä¸‹ï¼›è®­ç»ƒ/æ¨ç†å‡ä»è¯¥ç›®å½•åŠ è½½åˆ†è¯å™¨ã€‚

### ğŸ“¥ ä¸‹è½½é¢„è®­ç»ƒæƒé‡ä¸åˆ†è¯å™¨

å¦‚æœä½ ä¸æƒ³ä»å¤´è®­ç»ƒï¼Œå¯ä»¥ç›´æ¥ä» Hugging Face ä¸‹è½½é¢„è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡å’Œåˆ†è¯å™¨ï¼š

<a href="https://huggingface.co/caixiaoshun/mini-llm">
    <img alt="Hugging Face Model" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue">
</a>

ä½ å¯ä»¥å°†ä¸‹è½½çš„åˆ†è¯å™¨æ–‡ä»¶æ”¾ç½®åœ¨ `checkpoints/` ç›®å½•ä¸‹ã€‚

## ğŸ§±ğŸ§© æ¨¡å‹ç»“æ„æ¦‚è§ˆ

### ğŸ§± MiniLLM

æ–‡ä»¶ï¼š`src/models/components/mini_llm.py`

è®¾è®¡è¦ç‚¹ï¼š

- çº¯è§£ç è‡ªå›å½’æ¶æ„ï¼Œé‡‡ç”¨ Preâ€‘Normï¼ˆNorm â†’ å­å±‚ â†’ æ®‹å·®ï¼‰ä»¥æå‡ç¨³å®šæ€§ã€‚
- ç»„æŸ¥è¯¢æ³¨æ„åŠ› GQAï¼šé€šè¿‡å…±äº«/å¤ç”¨ KVï¼Œä»¥æ›´å°‘çš„ KV ç»„æœåŠ¡æ›´å¤š Q å¤´ï¼Œé™ä½æ˜¾å­˜ä¸è®¡ç®—ï¼›åŸºäºåŸç”Ÿ SDPA ä¸ä¸¥æ ¼å› æœæ©ç ã€‚
- æ—‹è½¬ä½ç½®ç¼–ç  RoPEï¼šä¸º Q/K æ³¨å…¥ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œå¢å¼ºé•¿ä¸Šä¸‹æ–‡æ³›åŒ–ä¸ä¸€å®šå¤–æ¨èƒ½åŠ›ã€‚
- å‰é¦ˆç½‘ç»œ FFNï¼šä¸¤å±‚ MLP + SiLU + Dropoutï¼Œæä¾›éçº¿æ€§ä¸ç‰¹å¾å˜æ¢ã€‚
- æƒé‡å…±äº«ï¼šè¾“å‡ºæŠ•å½±ä¸è¯åµŒå…¥å…±äº«ï¼Œé™ä½å‚æ•°é‡å¹¶å¸¦æ¥è½»å¾®æ­£åˆ™åŒ–ã€‚

è®­ç»ƒç›®æ ‡ä¸ºä¸‹ä¸€ä¸ª token çš„äº¤å‰ç†µï¼›å¯¹ padding åŠéç›‘ç£ç‰‡æ®µåš `-100` æ©ç ï¼Œé¿å…æ¢¯åº¦æ±¡æŸ“ã€‚

### ğŸ§© ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMiniMoEï¼‰

æ–‡ä»¶ï¼š`src/models/components/mini_moe.py`

è®¾è®¡è¦ç‚¹ï¼ˆä¸å…·ä½“è¶…å‚æ— å…³ï¼‰ï¼š

- ç¨€ç–æ›¿æ¢ï¼šä¿ç•™Denseæ¨¡å‹ä¸­çš„æ³¨æ„åŠ›ä¸å½’ä¸€åŒ–ï¼Œå°†æ¯å±‚ FFN æ›¿æ¢ä¸ºç¨€ç– MoE FFNï¼›ä»…è¢«è·¯ç”±é€‰ä¸­çš„ token ä¼šè¢«ä¸“å®¶å¤„ç†ï¼Œåœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹æå‡å®¹é‡ã€‚
- å¸¦å™ª Topâ€‘K è·¯ç”±ï¼šè·¯ç”±å™¨ç»™ token è¡¨ç¤ºæ‰“åˆ†å¹¶æ³¨å…¥å™ªå£°ï¼Œé¼“åŠ±æ¢ç´¢ä¸å¤šæ ·æ€§ï¼›é€‰ä¸­çš„ä¸“å®¶è¾“å‡ºæŒ‰é—¨æ§æƒé‡åŠ æƒæ±‚å’Œã€‚
- è´Ÿè½½å‡è¡¡ï¼ˆè¾…åŠ©æŸå¤±ï¼‰ï¼šæ­£åˆ™åŒ–è·¯ç”±åˆ†å¸ƒï¼Œä½¿ä¸“å®¶è·å¾—ç›¸è¿‘çš„â€œé‡è¦æ€§/æµé‡â€ï¼Œé˜²æ­¢åå¡Œï¼›è´¡çŒ®åˆ°è®­ç»ƒæŸå¤±ï¼Œæ¨ç†æ—¶å¯å…³é—­ã€‚
- ä¿ç•™æ®‹å·®è·¯å¾„ï¼šä¸“å®¶è¾“å‡ºé€šè¿‡æ®‹å·®å›å¹¶ï¼Œç¨³å®šæ¢¯åº¦ä¸ä¿¡æ¯æµã€‚

Lightning è®­ç»ƒæ¨¡å—ï¼š`src/models/mini_llm_module.py`

- ç»Ÿä¸€å°è£…ä¼˜åŒ–å™¨/è°ƒåº¦å™¨ã€è®­ç»ƒ/éªŒè¯/æµ‹è¯•å¾ªç¯ä¸æŒ‡æ ‡è®°å½•ã€‚
- è‡ªåŠ¨å¤„ç† MoE çš„â€œä¸»æŸå¤± + è¾…åŠ©æŸå¤±â€ç»„åˆï¼Œå¹¶é€‚é…åˆ†å¸ƒå¼/æ··ç²¾è®­ç»ƒã€‚

## ğŸ‹ï¸ è®­ç»ƒ

ä»ä»“åº“æ ¹ç›®å½•è¿è¡Œã€‚Lightning + Hydra é»˜è®¤å°†æ—¥å¿—å†™å…¥ `logs/`ã€‚

æç¤ºï¼šè‹¥ç”¨ GPUï¼Œå»ºè®®è¯•è¯• `configs/trainer/gpu.yaml`ï¼ˆbf16ï¼‰ã€‚ä¹Ÿå¯ç”¨ CLI è¦†ç›– `trainer.precision`ã€`trainer.devices`ã€`trainer.accumulate_grad_batches` ç­‰ã€‚

### ğŸ§¾ è„šæœ¬å¿«æ·æ–¹å¼ï¼ˆscripts/ï¼‰

`scripts/` ä¸‹æä¾› 4 ä¸ªä¸€é”® Bash è„šæœ¬ï¼Œæ³¨é‡Šä¸­ç»™å‡ºäº†ç­‰ä»·çš„ Python å‘½ä»¤ï¼Œä¾¿äºè‡ªå®šä¹‰ï¼š

```bash
# é¢„è®­ç»ƒï¼ˆDenseï¼‰
bash scripts/pretrain.sh
# ç­‰ä»·ï¼špython src/train.py experiment=pretrain logger=tensorboard

# SFTï¼ˆDenseï¼‰
bash scripts/sft.sh
# ç­‰ä»·ï¼špython src/train.py experiment=sft model.net.pretrain_ckpt="<path-to-your-pretrain-ckpt>" logger=tensorboard

# é¢„è®­ç»ƒï¼ˆMoEï¼‰
bash scripts/moe-pretrain.sh
# ç­‰ä»·ï¼špython src/train.py experiment=moe-pretrain logger=tensorboard

# SFTï¼ˆMoEï¼‰
bash scripts/moe-sft.sh
# ç­‰ä»·ï¼špython src/train.py experiment=moe-sft model.net.pretrain_ckpt="<path-to-your-MoE-pretrain-ckpt>" logger=tensorboard
```

è¯´æ˜ï¼šå¦‚éœ€è°ƒæ•´ batchã€ç²¾åº¦ã€è®¾å¤‡ç­‰ï¼Œå¯ç›´æ¥ç¼–è¾‘è„šæœ¬æˆ–ä½¿ç”¨ CLI è¦†ç›–ã€‚

### ğŸ“Š å‚è€ƒåŸºå‡†

ä»¥ä¸‹æ˜¯æˆ‘ä»¬å®éªŒçš„å‚è€ƒç»Ÿè®¡æ•°æ®ã€‚

- **ç¡¬ä»¶**: 8Ã— NVIDIA RTX 3090 (24GB æ˜¾å­˜)
- **ç²¾åº¦**: bf16-mixed (è„šæœ¬é»˜è®¤)

| æ¨¡å‹ | é˜¶æ®µ | è½®æ•° (Epochs) | æ—¶é•¿ |
| :--- | :--- | :--- | :--- |
| **mini-llm** | é¢„è®­ç»ƒ | 2 | 13å°æ—¶22åˆ†é’Ÿ |
| **mini-llm** | SFT | 2 | 2å°æ—¶56åˆ† |
| **mini-moe** | é¢„è®­ç»ƒ | 1 | 22å°æ—¶9åˆ†é’Ÿ |
| **mini-moe** | SFT | 2 | 13å°æ—¶ |

> **æ˜¾å­˜ä¸è¶³ï¼Ÿ**ï¼šå»ºè®®é™ä½ `data.batch_size` å¹¶å¢åŠ  `trainer.accumulate_grad_batches`ï¼Œæˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹é…ç½®ã€‚

### ğŸ¥£ è®­ç»ƒé…æ–¹ä¸å‚æ•°è¦†ç›–

å¯åŠ¨è®­ç»ƒçš„æœ€å°å‘½ä»¤ï¼š

```
python src/train.py experiment=pretrain logger=tensorboard
```

å¸¸ç”¨è¦†ç›–ç¤ºä¾‹ï¼š

- æ¨¡å‹å¤§å°ï¼š`model.net.config.num_layers`ã€`model.net.config.dim`ã€`model.net.config.num_heads`ã€`model.net.config.num_kv_groups`
- ååç›¸å…³ï¼š`data.batch_size`ã€`trainer.accumulate_grad_batches`ã€`trainer.devices`ã€`trainer.strategy`
- ç²¾åº¦/ç¨³å®šæ€§ï¼š`trainer.precision=bf16-true`ã€`trainer.grad_clip_val=1.0`

ç¤ºä¾‹ï¼ˆå°æ¨¡å‹ + bf16ï¼‰ï¼š

```bash
	trainer.precision=bf16-true
```

SFT éœ€è¦åŠ è½½é¢„è®­ç»ƒæƒé‡ã€‚å°† `model.net.pretrain_ckpt` è®¾ä¸ºä½ çš„Denseé¢„è®­ç»ƒæƒé‡ï¼š

```bash
python src/train.py experiment=sft \
	model.net.pretrain_ckpt="logs/train/mini-llm/pretrain/<timestamp>/checkpoints/<best-or-last>.ckpt" \
	logger=tensorboard
```

å…¶ä»–å¸¸ç”¨è¦†ç›–ï¼š`trainer.max_epochs`ã€`trainer.val_check_interval`ã€`data.max_seq_len`ã€`optim.lr`ã€‚

### ğŸ”§ MoE è®­ç»ƒï¼ˆé¢„è®­ç»ƒ â†’ SFTï¼‰

MoE è®­ç»ƒè¯·ä½¿ç”¨ `experiment=moe-pretrain` ä¸ `experiment=moe-sft`ï¼š

# é¢„è®­ç»ƒï¼ˆMoEï¼‰â€”â€”é€šå¸¸è¾ƒå° batchã€è¾ƒå¤§çš„ç´¯ç§¯æ­¥
```bash
python src/train.py experiment=moe-pretrain logger=tensorboard
```

```bash
# æ›´ä¿å®ˆçš„æ˜¾å­˜è®¾ç½®
python src/train.py experiment=moe-pretrain \
	data.batch_size=4 trainer.accumulate_grad_batches=8 trainer.precision=bf16-true

# å¿«é€Ÿè°ƒè¯•
python src/train.py experiment=moe-pretrain trainer.max_epochs=1 trainer.limit_train_batches=0.1
```

## ğŸ“ˆ è®­ç»ƒæ›²çº¿

ä¸‹å›¾å±•ç¤ºäº†ä¸åŒé˜¶æ®µçš„å‚è€ƒè®­ç»ƒæ›²çº¿ï¼š

<div align="center">
<table>
  <tr>
    <td align="center">
      <img src="log_pngs/mini-llm-pretrain.png" alt="mini-llm é¢„è®­ç»ƒæ›²çº¿" width="100%" />
      <br/>mini-llm é¢„è®­ç»ƒ
    </td>
    <td align="center">
      <img src="log_pngs/mini-llm-sft.png" alt="mini-llm SFT æ›²çº¿" width="100%" />
      <br/>mini-llm SFT
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="log_pngs/mini-moe-pretrain.png" alt="mini-moe é¢„è®­ç»ƒæ›²çº¿" width="100%" />
      <br/>mini-moe é¢„è®­ç»ƒ
    </td>
    <td align="center">
      <img src="log_pngs/mini-moe-sft.png" alt="mini-moe SFT æ›²çº¿" width="100%" />
      <br/>mini-moe SFT
    </td>
  </tr>
</table>
</div>

## ğŸš€ æ¨ç†ä¸ç¤ºä¾‹

<div align="center">
<img src="assets/result.png" alt="mini-llm è¿è¡Œæ¼”ç¤º" style="width: 100%; max-width: 800px; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" />
<p><em>mini-llm æ¨ç†æ¼”ç¤º â€”â€” å±•ç¤ºèŠå¤©ç•Œé¢ä¸æ¨¡å‹å›å¤</em></p>
</div>

ä¸¤ç§æ–¹å¼ï¼šç›´æ¥è°ƒç”¨ `MiniLLM`/`MiniMoE` çš„ `chat()`ï¼Œæˆ–è¿è¡Œ Streamlit èŠå¤©åº”ç”¨ã€‚

### ğŸ æ–¹å¼ Aï¼šPython è„šæœ¬

```python
import torch
import hydra
from omegaconf import OmegaConf
from transformers import PreTrainedTokenizerFast

tokenizer = PreTrainedTokenizerFast.from_pretrained("checkpoints")

# åŠ è½½Dense LLM é…ç½®å¹¶å®ä¾‹åŒ–
cfg = OmegaConf.load("configs/model/mini-llm.yaml")["net"]
model = hydra.utils.instantiate(cfg)  # MiniLLM(config)
model.load_ckpt("logs/train/mini-llm/sft/<timestamp>/checkpoints/<best>.ckpt")
model.eval().to("cuda" if torch.cuda.is_available() else "cpu")

conversations = [
	{"role": "user", "content": "è¯·ç”¨ä¸¤å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"}
]
print(model.chat(conversations, tokenizer, max_new_token=128, top_k=5))
```

MoE ç”¨æ³•ä¸€è‡´ï¼ˆå°† `mini-llm.yaml` æ›¿æ¢ä¸º `mini-moe.yaml`ï¼‰ã€‚è‹¥æ¨ç†æ—¶éœ€å…³é—­ MoE è¾…åŠ©æŸå¤±ï¼Œè¯·åœ¨åŠ è½½åè®¾ç½® `config.use_aux_loss = False`ã€‚

### ğŸ—¨ï¸ æ–¹å¼ Bï¼šStreamlit èŠå¤©åº”ç”¨ï¼ˆå·²æ›´æ–°ï¼‰

å†…ç½®ä¸¤ä¸ª UIï¼Œæ”¯æŒå¤šè½®å¯¹è¯ä¸æµå¼ç”Ÿæˆï¼š

```bash
# ğŸ¤– Denseæ¨¡å‹ UIï¼ˆéšè— system promptï¼Œå¼ºåŒ–åŠ©æ‰‹äººè®¾ï¼‰
streamlit run src/app/mini_llm_app.py

# ğŸ¤– MoE æ¨¡å‹ UIï¼ˆå¼ºåˆ¶å…³é—­æ¨ç†æ—¶è¾…åŠ©æŸå¤±ï¼‰
streamlit run src/app/mini_moe_app.py
```

ä¾§è¾¹æ åŠŸèƒ½ï¼ˆä¸¤è€…å…±æœ‰ï¼‰ï¼š

- ğŸ§® è®¾å¤‡ï¼ˆauto/cuda/cpuï¼‰ä¸ç²¾åº¦ï¼ˆauto/float16/bfloat16/float32ï¼‰
- ğŸ“ ç”Ÿæˆå‚æ•°ï¼šmax_new_tokensã€Topâ€‘Kã€temperatureã€seed
- âœ‚ï¸ truncate_ctx æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé¿å… OOMï¼‰
- ğŸ§¹ æ¸…ç©ºå†å²

å·®å¼‚ï¼š

- mini_llm_appï¼š
	- é€šè¿‡ `SYSTEM_PROMOT` æ³¨å…¥ç³»ç»Ÿäººè®¾ï¼ˆé¦–è½®éšè—ï¼Œç”¨äºæ¨ç†ä¸æ¸²æŸ“ï¼‰ã€‚
	- é»˜è®¤ï¼š`TOKENIZER_PATH = "checkpoints"`ï¼Œ`CONFIG_PATH = "configs/model/mini-llm.yaml"`ã€‚
- mini_moe_appï¼š
	- åœ¨åŠ è½½åè®¾ç½® `model_cfg["config"]["use_aux_loss"] = False`ï¼Œç¡®ä¿æ¨ç†ä¸è®¡ç®—è¾…åŠ©æŸå¤±ã€‚
	- é»˜è®¤ï¼š`truncate_ctx = 512`ï¼Œæ›´ä¿å®ˆçš„æ˜¾å­˜ä½¿ç”¨ã€‚

ä¸¤è€…å‡ä½¿ç”¨ `top_k_sample`ï¼Œå¹¶æ”¯æŒå¢é‡æ¸²æŸ“ã€‚è‹¥éœ€æ›´æ¢æƒé‡/åˆ†è¯å™¨ï¼Œè¯·ä¿®æ”¹é¡¶éƒ¨å¸¸é‡ï¼š`CONFIG_PATH`ã€`CKPT_PATH`ã€`TOKENIZER_PATH`ã€‚

## â˜ï¸ ä¸€é”®ä½“éªŒï¼ˆGoogle Colabï¼‰

åœ¨ Colab æ‰“å¼€æ¼”ç¤º Notebookï¼š

- mini-llmï¼ˆDenseï¼‰ï¼š[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1EwVictfkx6OUlu3yDuKfebvxeVUNEdbg?usp=sharing)
- mini-moeï¼ˆMoEï¼‰ï¼š[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1hoLR-S-YeCS01CVh4x8qg25cpSmGVcxu?usp=sharing)

å°æç¤ºï¼š

- è®°å¾—å¯ç”¨ GPUï¼ˆRuntime â†’ Change runtime type â†’ é€‰æ‹© T4/L4/A100 ç­‰ï¼‰ã€‚
- è‹¥ä¾èµ–æœªè‡ªåŠ¨å®‰è£…ï¼Œå¯æ‰§è¡Œä¸‹åˆ—å•å…ƒï¼ˆå¯é€‰ï¼‰ï¼š

```python
# Optional: clone repo and install deps
!git clone https://github.com/caixiaoshun/mini-llm.git
%cd mini-llm
!pip -q install -r requirements.txt
```

è‹¥ç½‘ç»œå—é™ï¼Œå¯è€ƒè™‘é•œåƒæºæˆ–å°†æ•°æ®æ”¾ç½®åœ¨ /contentï¼Œå¹¶åœ¨é…ç½®ä¸­ç›¸åº”ä¿®æ”¹è·¯å¾„ã€‚

## ğŸŒ åœ¨çº¿æ¼”ç¤ºï¼ˆHugging Face Spacesï¼‰


<a href="https://huggingface.co/spaces/caixiaoshun/mini-llm">
		<img alt="Open in Hugging Face Spaces" src="https://img.shields.io/badge/HuggingFace-Spaces-ffcc4d?logo=huggingface" />
</a>

è¯´æ˜ï¼š

- æä¾›ç®€æ´çš„ç½‘é¡µèŠå¤©ç•Œé¢ä»¥å¿«é€Ÿä½“éªŒã€‚
- ç”±äºèµ„æºé…é¢ï¼Œå†·å¯åŠ¨å¯èƒ½è¾ƒæ…¢ï¼›ç©ºé—²ä¼šè¿›å…¥ä¼‘çœ ã€‚
- è‹¥æ˜¾å­˜æœ‰é™ï¼Œè¯·å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦æˆ– `max_new_tokens`ï¼Œæˆ–é€‰æ‹© CPU/ä½ç²¾åº¦ã€‚

## ğŸ“Š è®­ç»ƒæ—¥å¿—ä¸å¯è§†åŒ–

- è®­ç»ƒæ—¥å¿—ä¸æ£€æŸ¥ç‚¹é»˜è®¤å†™å…¥ `logs/`ã€‚
- TensorBoardï¼šåœ¨å®éªŒ yaml ä¸­å¯ç”¨ `logger=tensorboard` åï¼Œè¿è¡Œï¼š

```bash
tensorboard --logdir logs
```

## ğŸ“„ æ•°æ®æ ¼å¼ä¸å¯¹é½è¯´æ˜

- é¢„è®­ç»ƒæ•°æ®ï¼š`data/mobvoi_seq_monkey_general_open_corpus.jsonl`ï¼Œæ¯è¡Œä¸€ä¸ª JSONï¼Œé”®ä¸º `text`ã€‚
	- æ•°æ®é›†å¤„ç†ï¼š`src/data/components/pretrain_dataset.py` é¢„ç½® BOSã€å·¦ç§»æ ‡ç­¾ï¼Œå¹¶ç”¨ `-100` åšæ©ç ã€‚
- SFT æ•°æ®ï¼š`data/train_3.5M_CN.json`ï¼Œæ¯è¡Œä¸€ä¸ª JSONï¼Œé”®ä¸º `conversations`ï¼ˆBelle/ShareGPT é£æ ¼ï¼‰ã€‚
	- æ•°æ®é›†å¤„ç†ï¼š`src/data/components/sft_dataset.py` è§„èŒƒåŒ– `{"from":"human"}` ä¸º `{"role":"user"}`ï¼Œ
		ä½¿ç”¨åˆ†è¯å™¨ `chat_template`ï¼Œä»…å¯¹ assistant token è®¡ç®—æŸå¤±ï¼ˆå…¶ä½™ç½®ä¸º `-100`ï¼‰ã€‚

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®ä»¥ Apache-2.0 è®¸å¯è¯å‘å¸ƒï¼ˆè§ `LICENSE`ï¼‰ã€‚

## ğŸ™ è‡´è°¢

<table>
<tr>
<td>
<h3>ğŸ“š æ•°æ®é›†</h3>
<p>
<a href="https://www.modelscope.cn/datasets/ddzhu123/seq-monkey"><img alt="ModelScope - seq-monkey" src="https://img.shields.io/badge/ModelScope-seq--monkey-0065FF?logo=modelscope&logoColor=white"></a>
<a href="https://huggingface.co/datasets/BelleGroup/train_3.5M_CN"><img alt="HF - BelleGroup/train_3.5M_CN" src="https://img.shields.io/badge/HF-BelleGroup%2Ftrain__3.5M__CN-FF6F00?logo=huggingface&logoColor=white"></a>
<br/>
<sub>æ„Ÿè°¢æ•°æ®ç¤¾åŒºæä¾›é«˜è´¨é‡å¼€æ”¾è¯­æ–™</sub>
<p>
</td>
<td>
<h3>ğŸ§° æ¡†æ¶ä¸åº“</h3>
<p>
<a href="https://pytorch.org/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white"></a>
<a href="https://lightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/Lightning-792EE5?logo=lightning&logoColor=white"></a>
<a href="https://github.com/huggingface/transformers"><img alt="Transformers" src="https://img.shields.io/badge/Transformers-FF6F00?logo=huggingface&logoColor=white"></a>
<a href="https://github.com/huggingface/datasets"><img alt="Datasets" src="https://img.shields.io/badge/Datasets-22A699?logo=python&logoColor=white"></a>
<a href="https://hydra.cc/"><img alt="Hydra" src="https://img.shields.io/badge/Hydra-000000?logo=dropbox-paper&logoColor=white"></a>
<br/>
<sub>æ„Ÿè°¢è¿™äº›ä¼˜ç§€çš„å¼€æºåŸºçŸ³</sub>
<p>
</td>
<td>
<h3>ğŸ§ª é¡¹ç›®æ¨¡æ¿</h3>
<p>
	<a href="https://github.com/ashleve/lightning-hydra-template">
		<img alt="Readme Card - lightning-hydra-template" src="https://github-readme-stats.vercel.app/api/pin/?username=ashleve&repo=lightning-hydra-template" />
	</a>
	<br/>
	<sub>æœ¬é¡¹ç›®åœ¨å…¶æ¨¡æ¿åŸºç¡€ä¸Šå®šåˆ¶ä¸æ‰©å±•</sub>
<p>
</td>
</tr>
</table>